{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hispanic_StyleGAN2_ada_kimg1000.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPI5E5y0pujD"
      },
      "source": [
        "# Custom Training StyleGan2-ADA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKYAU7Wub3WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edf5a25-3ee2-4b74-e6d7-54b10d0d44f9"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG9YnXPQmalc",
        "outputId": "1914d5f5-5484-48fa-9910-812e4a8f663d"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  7 06:37:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxxYlEKI9Gis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966d3490-2016-4363-e906-9d9778aa4280"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HX77jscX2zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152d06e3-61e3-4517-d2f6-93446294eb2d"
      },
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/AIB_04_Stylebot/colab-sg2-ada\"):\n",
        "    %cd \"/content/drive/MyDrive/AIB_04_Stylebot/colab-sg2-ada/stylegan2-ada\"\n",
        "else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/AIB_04_Stylebot\"\n",
        "    !mkdir colab-sg2-ada\n",
        "    %cd colab-sg2-ada\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada\n",
        "    %cd stylegan2-ada\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1zQGyqDeW63FEr3sXvFz23SVnqvoGEBUd/AIB_04_Stylebot/colab-sg2-ada/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hSjM8apL08B"
      },
      "source": [
        "# 필요한 라이브러리\n",
        "import cv2\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from google.colab import files\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "import os\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG_ir2s_Lpdn"
      },
      "source": [
        "# 1024 * 1024 size\n",
        "\n",
        "result_path = '/content/drive/MyDrive/AIB_04_Stylebot/selected_crop'\n",
        "\n",
        "image_files = Path(result_path)\n",
        "image_files = list(image_files.glob(r'*.jpg'))\n",
        "\n",
        "for name in image_files[:]:\n",
        "  img = cv2.imread(f'{name}')\n",
        "  img = cv2.resize(img, (1024,1024))\n",
        "  # 전처리 이미지 저장\n",
        "  cv2.imwrite(os.path.join(path , 'waka.jpg'), img)\n",
        "  # img.shape 변경 확인\n",
        "  print(img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeS9tDvt61VG"
      },
      "source": [
        "## Convert dataset to .tfrecords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q58MJbckLUc"
      },
      "source": [
        "##tfrecords\n",
        "TFRecord는 데이터 세트의 포맷의 하나로 TFRecord형식은 바이너리 코드의 시리즈를 저장하기 위한 단순한 형식\n",
        "\n",
        "==> 대규모 데이터를 효율적으로 학습할 수 있게 된다\n",
        "\n",
        "==> StyleGAN2-ADA가 읽을 수 있게 TFRecord로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G9lpycE4oLE",
        "outputId": "0025d075-6dba-4d44-850f-ac90f23321e1"
      },
      "source": [
        "# 파라미터 설명\n",
        "!python dataset_tool.py --help"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: dataset_tool.py [-h]\n",
            "                       {info,display,extract,compare,create_mnist,create_mnistrgb,create_cifar10,create_cifar100,create_svhn,create_lsun,create_lsun_wide,create_celeba,create_from_images,create_from_images_raw,create_from_image_folders,create_from_image_folders_raw,create_from_images_with_labels,create_from_hdf5,convert_to_hdf5,hdf5_from_images,unpack,pack,extract_brecahad_crops}\n",
            "                       ...\n",
            "\n",
            "Tool for creating multi-resolution TFRecords datasets for StyleGAN and ProGAN.\n",
            "\n",
            "positional arguments:\n",
            "  {info,display,extract,compare,create_mnist,create_mnistrgb,create_cifar10,create_cifar100,create_svhn,create_lsun,create_lsun_wide,create_celeba,create_from_images,create_from_images_raw,create_from_image_folders,create_from_image_folders_raw,create_from_images_with_labels,create_from_hdf5,convert_to_hdf5,hdf5_from_images,unpack,pack,extract_brecahad_crops}\n",
            "    info                Display general info about dataset.\n",
            "    display             Display images in dataset.\n",
            "    extract             Extract images from dataset.\n",
            "    compare             Compare two datasets.\n",
            "    create_mnist        Create dataset for MNIST.\n",
            "    create_mnistrgb     Create dataset for MNIST-RGB.\n",
            "    create_cifar10      Create dataset for CIFAR-10.\n",
            "    create_cifar100     Create dataset for CIFAR-100.\n",
            "    create_svhn         Create dataset for SVHN.\n",
            "    create_lsun         Create dataset for single LSUN category.\n",
            "    create_lsun_wide    Create LSUN dataset with non-square aspect ratio.\n",
            "    create_celeba       Create dataset for CelebA.\n",
            "    create_from_images  Create dataset from a directory full of images.\n",
            "    create_from_images_raw\n",
            "                        Create dataset from a directory full of images. Please\n",
            "                        be carefulsince the tool recursively searches inside\n",
            "                        every sub-directory for image files\n",
            "    create_from_image_folders\n",
            "                        Create dataset from a directory full of images. Use\n",
            "                        the folders to generate labels.\n",
            "    create_from_image_folders_raw\n",
            "                        Create dataset from a directory full of images in raw\n",
            "                        format. Use the folders to generate labels.\n",
            "    create_from_images_with_labels\n",
            "                        Create dataset from a directory full of images and\n",
            "                        class labels.\n",
            "    create_from_hdf5    Create dataset from legacy HDF5 archive.\n",
            "    convert_to_hdf5     Convert dataset to legacy HDF5 archive.\n",
            "    hdf5_from_images    Create HDF5 archive from a directory of images.\n",
            "    unpack              Unpack a TFRecords dataset to labels and images for\n",
            "                        later repackaging with `pack`.\n",
            "    pack                Repackage an unpacked dataset into TFRecords.\n",
            "    extract_brecahad_crops\n",
            "                        Extract crops from the original BreCaHAD images\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "Type \"dataset_tool.py <command> -h\" for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-BZHhBe7AvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46b7547-7deb-4984-9065-d8af3c8c30d2"
      },
      "source": [
        "!python dataset_tool.py create_from_images /content/drive/MyDrive/AIB_04_Stylebot/colab-sg2-ada/stylegan2-ada/datasets/{'1024image'} {result_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "Loading images from \"/content/drive/MyDrive/AIB_04_Stylebot/selected_crop\"\n",
            "Creating dataset \"/content/drive/MyDrive/AIB_04_Stylebot/colab-sg2-ada/stylegan2-ada/datasets/1024image\"\n",
            "dataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 55 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DvTupHzP2s_"
      },
      "source": [
        "## Train a custom model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxu7CA0Qb1Yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5c9ecd-3769-4896-f8f3-84600632982b"
      },
      "source": [
        "# 파라미터 설명\n",
        "!python train.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
            "                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n",
            "                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n",
            "                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n",
            "                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n",
            "                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n",
            "                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n",
            "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
            "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
            "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
            "\n",
            "Train a GAN using the techniques described in the paper\n",
            "\"Training Generative Adversarial Networks with Limited Data\".\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "general options:\n",
            "  --outdir DIR          Where to save the results (required)\n",
            "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
            "  --snap INT            Snapshot interval (default: 50 ticks)\n",
            "  --seed INT            Random seed (default: 1000)\n",
            "  -n, --dry-run         Print training options and exit\n",
            "\n",
            "training dataset:\n",
            "  --data PATH           Training dataset path (required)\n",
            "  --res INT             Dataset resolution (default: highest available)\n",
            "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
            "  --mirrory BOOL        Augment dataset with y-flips (default: false)\n",
            "  --use-raw BOOL        Use raw image dataset, i.e. created from\n",
            "                        create_from_images_raw (default: False)\n",
            "\n",
            "metrics:\n",
            "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
            "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
            "\n",
            "base config:\n",
            "  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n",
            "                        Base config (default: auto)\n",
            "  --lrate FLOAT         Override learning rate\n",
            "  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n",
            "                        for discriminator) (default: false)\n",
            "  --gamma FLOAT         Override R1 gamma\n",
            "  --nkimg INT           Override starting count\n",
            "  --kimg INT            Override training duration\n",
            "  --topk FLOAT          utilize top-k training\n",
            "\n",
            "discriminator augmentation:\n",
            "  --aug {noaug,ada,fixed,adarv}\n",
            "                        Augmentation mode (default: ada)\n",
            "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
            "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
            "  --initstrength INITSTRENGTH\n",
            "                        Override ADA strength at start\n",
            "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
            "                        Augmentation pipeline (default: bgc)\n",
            "\n",
            "comparison methods:\n",
            "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
            "                        Comparison method (default: nocmethod)\n",
            "  --dcap FLOAT          Multiplier for discriminator capacity\n",
            "\n",
            "transfer learning:\n",
            "  --resume RESUME       Resume from network pickle (default: noresume)\n",
            "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
            "\n",
            "examples:\n",
            "\n",
            "  # Train custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
            "      --cfg=cifar\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
            "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
            "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
            "\n",
            "available base configs (--cfg):\n",
            "  auto           Automatically select reasonable defaults based on resolution\n",
            "                 and GPU count. Good starting point for new datasets.\n",
            "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
            "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
            "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
            "\n",
            "transfer learning source networks (--resume):\n",
            "  ffhq256        FFHQ trained at 256x256 resolution.\n",
            "  ffhq512        FFHQ trained at 512x512 resolution.\n",
            "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n",
            "  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n",
            "  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n",
            "  brecahad512    BreCaHAD trained at 512x512 resolution.\n",
            "  cifar10        CIFAR10 trained at 32x32 resolution.\n",
            "  metfaces512    MetFaces trained at 512x512 resolution.\n",
            "  <path or URL>  Custom network pickle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOftFoyiDU3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f078f685-cc2c-4698-c9a0-24c4d9ccab58"
      },
      "source": [
        "#this name must EXACTLY match the dataset name you used when creating the .tfrecords file\n",
        "dataset_name = \"1024image\"\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 4\n",
        "#should the images be mirrored left to right?\n",
        "mirrored = True\n",
        "#should the images be mirrored top to bottom?\n",
        "mirroredY = False\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#augments\n",
        "augs = \"bg\"\n",
        "'''\n",
        "augpipe_specs = {\n",
        "        'blit':     dict(xflip=1, rotate90=1, xint=1),\n",
        "        'geom':     dict(scale=1, rotate=1, aniso=1, xfrac=1),\n",
        "        'color':    dict(brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1),\n",
        "        'filter':   dict(imgfilter=1),\n",
        "        'noise':    dict(noise=1),\n",
        "        'cutout':   dict(cutout=1),\n",
        "        'bg':       dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1),\n",
        "        'bgc':      dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1),\n",
        "        'bgcf':     dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1),\n",
        "        'bgcfn':    dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1, noise=1),\n",
        "        'bgcfnc':   dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1, noise=1, cutout=1),\n",
        "    }\n",
        "\n",
        "# Discriminator augmentation. = Generator는 real data의 분포를 학습하기 때문에\n",
        "aug        = None, # Augmentation mode: 'ada' (default), 'noaug', 'fixed', 'adarv'\n",
        "p          = None, # Specify p for 'fixed' (required): <float>\n",
        "target     = None, # Override ADA target for 'ada' and 'adarv': <float>, default = depends on aug\n",
        "augpipe    = None, # Augmentation pipeline: 'blit', 'geom', 'color', 'filter', 'noise', 'cutout', 'bg', 'bgc' (default), ..., 'bgcfnc'\n",
        "\n",
        "# Transfer learning.\n",
        "resume     = None, # Load previous network: 'noresume' (default), 'ffhq256', 'ffhq512', 'ffhq1024', 'celebahq256', 'lsundog256', <file>, <url>\n",
        "'''\n",
        "# Transfer learning.\n",
        "resume_from = \"ffhq1024\" \n",
        "\n",
        "#kimg = batch?\n",
        "!python train.py --outdir ./results --snap={snapshot_count} --cfg=stylegan2 --kimg=100 --data=./datasets/{dataset_name} --augpipe={augs} --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --resume={resume_from}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x558350a92000 @  0x7f97f48ce001 0x7f97f1ad154f 0x7f97f1b21b58 0x7f97f1b25b17 0x7f97f1bc4203 0x558349a22544 0x558349a22240 0x558349a96627 0x558349a90ced 0x558349a2448c 0x558349a65159 0x558349a620a4 0x558349a22d49 0x558349a9694f 0x558349a909ee 0x558349962e2b 0x558349a92fe4 0x558349a909ee 0x558349962e2b 0x558349a92fe4 0x558349a90ced 0x558349962e2b 0x558349a92fe4 0x558349a23afa 0x558349a91915 0x558349a909ee 0x558349a906f3 0x558349b5a4c2 0x558349b5a83d 0x558349b5a6e6 0x558349b32163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x558450a92000 @  0x7f97f48cc1e7 0x7f97f1ad146e 0x7f97f1b21c7b 0x7f97f1b2235f 0x7f97f1bc4103 0x558349a22544 0x558349a22240 0x558349a96627 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a23afa 0x558349a91915 0x558349a909ee 0x558349a23bda 0x558349a95d00 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a90ced 0x558349a2448c 0x558349a65159 0x558349a620a4 0x558349a22d49 0x558349a9694f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x558552342000 @  0x7f97f48cc1e7 0x7f97f1ad146e 0x7f97f1b21c7b 0x7f97f1b2235f 0x7f979d4d1235 0x7f979ce54792 0x7f979ce54d42 0x7f979ce0daee 0x558349a22437 0x558349a22240 0x558349a960f3 0x558349a23afa 0x558349a91c0d 0x558349a90ced 0x558349962eb0 0x558349a92fe4 0x558349a909ee 0x558349a23bda 0x558349a91c0d 0x558349a90ced 0x558349a23bda 0x558349a91c0d 0x558349a23afa 0x558349a91c0d 0x558349a909ee 0x558349a24271 0x558349a24698 0x558349a92fe4 0x558349a909ee 0x558349a23bda 0x558349a91915\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 8,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 10\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"xflip\": 1,\n",
            "      \"rotate90\": 1,\n",
            "      \"xint\": 1,\n",
            "      \"scale\": 1,\n",
            "      \"rotate\": 1,\n",
            "      \"aniso\": 1,\n",
            "      \"xfrac\": 1,\n",
            "      \"brightness\": 1,\n",
            "      \"contrast\": 1,\n",
            "      \"lumaflip\": 1,\n",
            "      \"hue\": 1,\n",
            "      \"saturation\": 1\n",
            "    },\n",
            "    \"tune_kimg\": 100\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 4,\n",
            "  \"network_snapshot_ticks\": 4,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"./datasets/new_image\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 512,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"metric_arg_list\": [],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"./datasets/new_image\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 512,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"total_kimg\": 100,\n",
            "  \"minibatch_size\": 32,\n",
            "  \"minibatch_gpu\": 4,\n",
            "  \"G_smoothing_kimg\": 10,\n",
            "  \"G_smoothing_rampup\": null,\n",
            "  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl\",\n",
            "  \"run_dir\": \"./results/00013-new_image-mirror-stylegan2-kimg100-bgc-resumeffhq1024\"\n",
            "}\n",
            "\n",
            "Output directory:  ./results/00013-new_image-mirror-stylegan2-kimg100-bgc-resumeffhq1024\n",
            "Training data:     ./datasets/new_image\n",
            "Training length:   100 kimg\n",
            "Resolution:        512\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x5583509d0000 @  0x7f97f48ce001 0x7f97f1ad154f 0x7f97f1b21b58 0x7f97f1b25b17 0x7f97f1bc4203 0x558349a22544 0x558349a22240 0x558349a96627 0x558349a90ced 0x558349a2448c 0x558349a65159 0x558349a620a4 0x558349a22d49 0x558349a9694f 0x558349a909ee 0x558349962e2b 0x558349a92fe4 0x558349a909ee 0x558349962e2b 0x558349a92fe4 0x558349a90ced 0x558349962e2b 0x558349a92fe4 0x558349a23afa 0x558349a91915 0x558349a909ee 0x558349a906f3 0x558349b5a4c2 0x558349b5a83d 0x558349b5a6e6 0x558349b32163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x558652342000 @  0x7f97f48cc1e7 0x7f97f1ad146e 0x7f97f1b21c7b 0x7f97f1b2235f 0x7f97f1bc4103 0x558349a22544 0x558349a22240 0x558349a96627 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a23afa 0x558349a91915 0x558349a909ee 0x558349a23bda 0x558349a95d00 0x558349a909ee 0x558349a23bda 0x558349a92737 0x558349a90ced 0x558349a2448c 0x558349a65159 0x558349a620a4 0x558349a22d49 0x558349a9694f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x558652342000 @  0x7f97f48cc1e7 0x7f97f1ad146e 0x7f97f1b21c7b 0x7f97f1b2235f 0x7f979d4d1235 0x7f979ce54792 0x7f979ce54d42 0x7f979ce0daee 0x558349a22437 0x558349a22240 0x558349a960f3 0x558349a23afa 0x558349a91c0d 0x558349a90ced 0x558349962eb0 0x558349a92fe4 0x558349a909ee 0x558349a23bda 0x558349a91c0d 0x558349a90ced 0x558349a23bda 0x558349a91c0d 0x558349a23afa 0x558349a91c0d 0x558349a909ee 0x558349a24271 0x558349a24698 0x558349a92fe4 0x558349a909ee 0x558349a23bda 0x558349a91915\n",
            "Image shape: [3, 512, 512]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Resuming from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl\"\n",
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl ... done\n",
            "\n",
            "G                             Params    OutputShape         WeightShape     \n",
            "---                           ---       ---                 ---             \n",
            "latents_in                    -         (?, 512)            -               \n",
            "labels_in                     -         (?, 0)              -               \n",
            "epochs                        1         ()                  ()              \n",
            "epochs_1                      1         ()                  ()              \n",
            "G_mapping/Normalize           -         (?, 512)            -               \n",
            "G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense2              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense3              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense4              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense5              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense6              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense7              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Broadcast           -         (?, 16, 512)        -               \n",
            "dlatent_avg                   -         (512,)              -               \n",
            "Truncation/Lerp               -         (?, 16, 512)        -               \n",
            "G_synthesis/4x4/Const         8192      (?, 512, 4, 4)      (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv          2622465   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB         264195    (?, 3, 4, 4)        (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up      2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1         2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample      -         (?, 3, 8, 8)        -               \n",
            "G_synthesis/8x8/ToRGB         264195    (?, 3, 8, 8)        (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up    2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1       2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample    -         (?, 3, 16, 16)      -               \n",
            "G_synthesis/16x16/ToRGB       264195    (?, 3, 16, 16)      (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up    2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1       2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample    -         (?, 3, 32, 32)      -               \n",
            "G_synthesis/32x32/ToRGB       264195    (?, 3, 32, 32)      (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up    2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1       2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample    -         (?, 3, 64, 64)      -               \n",
            "G_synthesis/64x64/ToRGB       264195    (?, 3, 64, 64)      (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up  1442561   (?, 256, 128, 128)  (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1     721409    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample  -         (?, 3, 128, 128)    -               \n",
            "G_synthesis/128x128/ToRGB     132099    (?, 3, 128, 128)    (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up  426369    (?, 128, 256, 256)  (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1     213249    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample  -         (?, 3, 256, 256)    -               \n",
            "G_synthesis/256x256/ToRGB     66051     (?, 3, 256, 256)    (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up  139457    (?, 64, 512, 512)   (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1     69761     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample  -         (?, 3, 512, 512)    -               \n",
            "G_synthesis/512x512/ToRGB     33027     (?, 3, 512, 512)    (1, 1, 64, 3)   \n",
            "---                           ---       ---                 ---             \n",
            "Total                         30276585                                      \n",
            "\n",
            "\n",
            "D                    Params    OutputShape         WeightShape     \n",
            "---                  ---       ---                 ---             \n",
            "images_in            -         (?, 3, 512, 512)    -               \n",
            "labels_in            -         (?, 0)              -               \n",
            "512x512/FromRGB      256       (?, 64, 512, 512)   (1, 1, 3, 64)   \n",
            "512x512/Conv0        36928     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
            "512x512/Conv1_down   73856     (?, 128, 256, 256)  (3, 3, 64, 128) \n",
            "512x512/Skip         8192      (?, 128, 256, 256)  (1, 1, 64, 128) \n",
            "256x256/Conv0        147584    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
            "256x256/Conv1_down   295168    (?, 256, 128, 128)  (3, 3, 128, 256)\n",
            "256x256/Skip         32768     (?, 256, 128, 128)  (1, 1, 128, 256)\n",
            "128x128/Conv0        590080    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "128x128/Conv1_down   1180160   (?, 512, 64, 64)    (3, 3, 256, 512)\n",
            "128x128/Skip         131072    (?, 512, 64, 64)    (1, 1, 256, 512)\n",
            "64x64/Conv0          2359808   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "64x64/Conv1_down     2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "64x64/Skip           262144    (?, 512, 32, 32)    (1, 1, 512, 512)\n",
            "32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "32x32/Skip           262144    (?, 512, 16, 16)    (1, 1, 512, 512)\n",
            "16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "16x16/Skip           262144    (?, 512, 8, 8)      (1, 1, 512, 512)\n",
            "8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "8x8/Skip             262144    (?, 512, 4, 4)      (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n",
            "4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n",
            "4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n",
            "Output               513       (?, 1)              (512, 1)        \n",
            "---                  ---       ---                 ---             \n",
            "Total                28982849                                      \n",
            "\n",
            "Exporting sample images...\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "Initializing metrics...\n",
            "Training for 100 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      time 2m 02s       sec/tick 55.5    sec/kimg 433.29  maintenance 66.9   gpumem 7.1   augment 0.000\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 645, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 637, in main\n",
            "    run_training(**vars(args))\n",
            "  File \"train.py\", line 522, in run_training\n",
            "    training_loop.training_loop(**training_options)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1zQGyqDeW63FEr3sXvFz23SVnqvoGEBUd/AIB_04_Stylebot/colab-sg2-ada/stylegan2-ada/training/training_loop.py\", line 262, in training_loop\n",
            "    tflib.run(G_train_op)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1zQGyqDeW63FEr3sXvFz23SVnqvoGEBUd/AIB_04_Stylebot/colab-sg2-ada/stylegan2-ada/dnnlib/tflib/tfutil.py\", line 33, in run\n",
            "    return tf.get_default_session().run(*args, **kwargs)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7-AuMqIGHHf"
      },
      "source": [
        "# **Generate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBPADzltPHhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f450c74e-e162-4b03-883f-25ab1f37b092"
      },
      "source": [
        "!pip install opensimplex"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opensimplex\n",
            "  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opensimplex\n",
            "Successfully installed opensimplex-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz_yWHYOQ-EI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a39b191-39e3-4305-bf75-c3887feda826"
      },
      "source": [
        "!python generate.py generate-images --help"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: generate.py generate-images [-h] --network NETWORK_PKL --seeds SEEDS\n",
            "                                   [--trunc TRUNCATION_PSI]\n",
            "                                   [--class CLASS_IDX] [--create-grid]\n",
            "                                   [--outdir DIR] [--save_vector] [--fixnoise]\n",
            "                                   [--jpg_quality JPG_QUALITY]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --network NETWORK_PKL\n",
            "                        Network pickle filename\n",
            "  --seeds SEEDS         List of random seeds\n",
            "  --trunc TRUNCATION_PSI\n",
            "                        Truncation psi (default: 0.5)\n",
            "  --class CLASS_IDX     Class label (default: unconditional)\n",
            "  --create-grid         Add flag to save the generated images in a grid\n",
            "  --outdir DIR          Root directory for run results (default: out)\n",
            "  --save_vector         also save vector in .npy format\n",
            "  --fixnoise            generate images using fixed noise (more accurate for\n",
            "                        interpolations)\n",
            "  --jpg_quality JPG_QUALITY\n",
            "                        Quality compression for JPG exports (1 to 95), keep\n",
            "                        default value to export as PNG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekTD-s5OIApP"
      },
      "source": [
        "network = '/content/drive/MyDrive/AIB_04_Stylebot/network-snapshot-000200.pkl'\n",
        "\n",
        "#--create-grid\n",
        "'''\n",
        "Seed must be between 0 and 2**32 - 1\n",
        "\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
        "    tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
        "    images = Gs.run(z, label, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "    PIL.Image.fromarray(images[0], 'RGB').save(f'{outdir}/seed{seed:04d}.png')\n",
        "'''\n",
        "\n",
        "!python generate.py generate-images --outdir='/content/image' --trunc=0 --seeds=0-555 --network={network} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyyVrgjJSXGq",
        "outputId": "9f7ad940-752a-4989-c09f-1fd3da6d6637"
      },
      "source": [
        "!python style_mixing.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: style_mixing.py [-h] --network NETWORK_PKL --rows ROW_SEEDS --cols\n",
            "                       COL_SEEDS [--styles COL_STYLES]\n",
            "                       [--trunc TRUNCATION_PSI] --outdir DIR\n",
            "\n",
            "Generate style mixing image matrix using pretrained network pickle.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --network NETWORK_PKL\n",
            "                        Network pickle filename\n",
            "  --rows ROW_SEEDS      Random seeds to use for image rows\n",
            "  --cols COL_SEEDS      Random seeds to use for image columns\n",
            "  --styles COL_STYLES   Style layer range (default: 0-6)\n",
            "  --trunc TRUNCATION_PSI\n",
            "                        Truncation psi (default: 0.5)\n",
            "  --outdir DIR          Where to save the output images\n",
            "\n",
            "examples:\n",
            "\n",
            "  python style_mixing.py --outdir=out --trunc=1 --rows=85,100,75,458,1500 --cols=55,821,1789,293 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1QhJ03ySerC",
        "outputId": "1665d9a1-f81d-44bd-cf88-27d0dc8d1d13"
      },
      "source": [
        "!python style_mixing.py --outdir='/content/drive/MyDrive/AIB_04_Stylebot/colab-sg2-ada/style_mixing/style=15-18 1024' --trunc=0.5 --rows=4585 --cols=3333,1548,5184,6954,4878  --network='/content/drive/MyDrive/AIB_04_Stylebot/network-snapshot-000100.pkl' --styles=0-3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/AIB_04_Stylebot/network-snapshot-000100.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Generating W vectors...\n",
            "Generating images...\n",
            "Generating style-mixed images...\n",
            "Saving images...\n",
            "Saving image grid...\n"
          ]
        }
      ]
    }
  ]
}